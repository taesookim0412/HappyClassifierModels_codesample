{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import app\n",
    "import Models.pytorch_joy_and_anger.joy_and_anger_utils as utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Establish our pytorch factory methods we are trying to reverse engineer from python code into java code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pt_tokenizer = get_tokenizer(\"basic_english\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_patterns = [r'\\'',\n",
    "             r'\\\"',\n",
    "             r'\\.',\n",
    "             r'<br \\/>',\n",
    "             r',',\n",
    "             r'\\(',\n",
    "             r'\\)',\n",
    "             r'\\!',\n",
    "             r'\\?',\n",
    "             r'\\;',\n",
    "             r'\\:',\n",
    "             r'\\s+']\n",
    "\n",
    "_replacements = [' \\'  ',\n",
    "                 '',\n",
    "                 ' . ',\n",
    "                 ' ',\n",
    "                 ' , ',\n",
    "                 ' ( ',\n",
    "                 ' ) ',\n",
    "                 ' ! ',\n",
    "                 ' ? ',\n",
    "                 ' ',\n",
    "                 ' ',\n",
    "                 ' ']\n",
    "\n",
    "_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[(re.compile(r\"\\'\", re.UNICODE), \" '  \"),\n (re.compile(r'\\\"', re.UNICODE), ''),\n (re.compile(r'\\.', re.UNICODE), ' . '),\n (re.compile(r'<br \\/>', re.UNICODE), ' '),\n (re.compile(r',', re.UNICODE), ' , '),\n (re.compile(r'\\(', re.UNICODE), ' ( '),\n (re.compile(r'\\)', re.UNICODE), ' ) '),\n (re.compile(r'\\!', re.UNICODE), ' ! '),\n (re.compile(r'\\?', re.UNICODE), ' ? '),\n (re.compile(r'\\;', re.UNICODE), ' '),\n (re.compile(r'\\:', re.UNICODE), ' '),\n (re.compile(r'\\s+', re.UNICODE), ' ')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_patterns_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def _basic_english_normalize(line):\n",
    "    r\"\"\"\n",
    "    Basic normalization for a line of text.\n",
    "    Normalization includes\n",
    "    - lowercasing\n",
    "    - complete some basic text normalization for En glish words as follows:\n",
    "        add spaces before and after '\\''\n",
    "        remove '\\\"',\n",
    "        add spaces before and after '.'\n",
    "        replace '<br \\/>'with single space\n",
    "        add spaces before and after ','\n",
    "        add spaces before and after '('\n",
    "        add spaces before and after ')'\n",
    "        add spaces before and after '!'\n",
    "        add spaces before and after '?'\n",
    "        replace ';' with single space\n",
    "        replace ':' with single space\n",
    "        replace multiple spaces with single space\n",
    "\n",
    "    Returns a list of tokens after splitting on whitespace.\n",
    "    \"\"\"\n",
    "\n",
    "    line = line.lower()\n",
    "    for pattern_re, replaced_str in _patterns_dict:\n",
    "        line = pattern_re.sub(replaced_str, line)\n",
    "    return line.split()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tokenizer = lambda line: _basic_english_normalize(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['hello', ',', 'world']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello, world\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 7520 items\n",
      "{'joy': 0.0, 'anger': 1.0}\n",
      "('im grabbing a minute to post i feel greedy wrong', 1)\n"
     ]
    }
   ],
   "source": [
    "train_ds = utils.HappyClassifierDataset(\"train.txt\", probabilistic=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Create a test method in similar syntax to java\n",
    "def test_tokenizer(pt_pipeline, deciphered_pipeline, ds):\n",
    "    accuracy = 0\n",
    "    total_count = 0\n",
    "    for i, (text, label) in enumerate(ds.train_data):\n",
    "        pt_text = pt_pipeline(text)\n",
    "        new_text = deciphered_pipeline(text)\n",
    "\n",
    "        # assert pt_text array equals new_text array\n",
    "        accuracy += (pt_text == new_text)\n",
    "        #print(pt_text, new_text)\n",
    "        total_count += 1\n",
    "        if i == 0:\n",
    "            print(new_text)\n",
    "\n",
    "        if (i + 1) % (len(ds.train_data) // 5) == 0:\n",
    "            print(f\"Iteration {i} | Accuracy: {accuracy / total_count} %.\")\n",
    "        if accuracy != total_count:\n",
    "            print(pt_text, new_text)\n",
    "            break\n",
    "    try:\n",
    "        assert(accuracy == total_count)\n",
    "    except AssertionError:\n",
    "        print(\"Not the same\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'grabbing', 'a', 'minute', 'to', 'post', 'i', 'feel', 'greedy', 'wrong']\n",
      "Iteration 1503 | Accuracy: 1.0 %.\n",
      "Iteration 3007 | Accuracy: 1.0 %.\n",
      "Iteration 4511 | Accuracy: 1.0 %.\n",
      "Iteration 6015 | Accuracy: 1.0 %.\n",
      "Iteration 7519 | Accuracy: 1.0 %.\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer(pt_tokenizer, tokenizer, train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(['a', 'b', 'c'] == ['a', 'b'])\n",
    "print(['a', 'b', 'c'] == ['a', 'b', 'c'])\n",
    "print(['ab'] == ['a', 'b'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenizer looks good"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now create the vocab..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(list(map(lambda k: tokenizer(k), [txt for txt, label in train_ds.train_data])), specials=[\"<unk>\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[353, 96, 0, 171]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['great', 'day', \"we're\", 'having'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "vocab_pipeline = lambda sentence: vocab(tokenizer(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3825, 0, 191, 0]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_pipeline(\"Hello, world!\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import collections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# example input: normalized_sentence = ['im', 'grabbing', 'a', 'minute', 'to', 'post', 'i', 'feel', 'greedy', 'wrong'],\n",
    "# specials = [\"<unk>\"]\n",
    "def create_vocab(word_frequencies: dict[str, int], specials, min_freq = 1, special_first=True):\n",
    "    '''\n",
    "    Removes specials and puts them at the front or the beginning.\n",
    "    Filters out words that do not fill min_freq requirements.\n",
    "    :param word_frequencies: map of {word: freq}\n",
    "    :param specials: list of ['<unk>'] specials\n",
    "    :param min_freq: minimum frequency the word has to appear in our vocabulary\n",
    "    :param special_first: whether specials are most common in our vocab or not.\n",
    "    :return: dict of { word: freq }\n",
    "    '''\n",
    "    tokens = []\n",
    "\n",
    "    if special_first:\n",
    "        tokens.extend(specials)\n",
    "\n",
    "    specials_set = set(specials)\n",
    "\n",
    "    for word, freq in word_frequencies.items():\n",
    "        if freq >= min_freq and word not in specials_set:\n",
    "            tokens.append(word)\n",
    "\n",
    "    if special_first is False:\n",
    "        tokens.extend(specials)\n",
    "\n",
    "    res = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        res[token] = i\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def build_vocab_from_iterator_custom(normalized_sentences_list: list[list[str]], specials: list[str] = [\"<unk>\"]):\n",
    "    '''\n",
    "    Returns a map of {token: freq}. Depending on if we specify special first and min frequency we obtain a different result map.\n",
    "    :param normalized_sentences_list: List of sentences that have been tokenized. For example, [['this', ',' 'sentence'], ['hello',',','world']]\n",
    "    :param specials: list of specials\n",
    "    :return: map of { token : freq }\n",
    "    '''\n",
    "    word_frequencies = {}\n",
    "    for sentence in normalized_sentences_list:\n",
    "        for word in sentence:\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "    # sort by descending frequencies then lexicographically.\n",
    "    word_frequencies = dict(sorted(word_frequencies.items(), key=lambda x: (-1 * x[1], x[0])))\n",
    "\n",
    "    return create_vocab(word_frequencies, specials)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "build_vocab_from_iterator_custom(list(map(lambda k: tokenizer(k), [txt for txt, label in train_ds.train_data])), specials=[\"<unk>\"])\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def build_vocab_from_tokenized_sentences_optimized(normalized_sentences_list: list[list[str]], specials: list[str] = [\"<unk>\"]):\n",
    "    word_frequencies = {}\n",
    "    specials_set = set(specials)\n",
    "    max_freq = -1\n",
    "    for sentence in normalized_sentences_list:\n",
    "        for word in sentence:\n",
    "            if word not in specials_set:\n",
    "                word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "                max_freq = max(max_freq, word_frequencies[word])\n",
    "    biggest_freq_after = max_freq + 1\n",
    "    for special in specials:\n",
    "        word_frequencies[special] = biggest_freq_after\n",
    "        biggest_freq_after += 1\n",
    "\n",
    "    word_frequencies_sorted = {k: v for k, v in sorted(word_frequencies.items(), key=lambda x: (-x[1], x[0]))}\n",
    "    res = {}\n",
    "    for i, (word, freq) in enumerate(word_frequencies_sorted.items()):\n",
    "        res[word] = i\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "build_vocab_from_tokenized_sentences_optimized(list(map(lambda k: tokenizer(k), [txt for txt, label in train_ds.train_data])), specials=[\"<unk>\"])\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Created our functions, let's compare them all, some repetition here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[353, 96, 0, 171]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab_from_factory = build_vocab_from_iterator(list(map(lambda k: tokenizer(k), [txt for txt, label in train_ds.train_data])),\n",
    "                                  specials=[\"<unk>\"])\n",
    "vocab_from_factory.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_from_factory(['great', 'day', \"we're\", 'having'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "vocab_from_custom_detailed = build_vocab_from_iterator_custom(list(map(lambda k: tokenizer(k), [txt for txt, label in train_ds.train_data])))\n",
    "def vocab_from_custom_lambda(vocab_map, tokenized):\n",
    "    res = []\n",
    "    for token in tokenized:\n",
    "        res.append(vocab_map.get(token, 0))\n",
    "    return res\n",
    "vocab_from_custom_detailed_pipeline = lambda sentence: vocab_from_custom_lambda(vocab_from_custom_detailed, tokenizer(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[3825, 0, 191, 0]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_from_custom_detailed_pipeline(\"Hello, world!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "vocab_from_custom_optimized = build_vocab_from_tokenized_sentences_optimized(list(map(lambda k: _basic_english_normalize(k), [txt for txt, label in train_ds.train_data])))\n",
    "vocab_from_custom_optimized_pipeline = lambda sentence: vocab_from_custom_lambda(vocab_from_custom_optimized, tokenizer(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[3825, 0, 191, 0]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_from_custom_optimized_pipeline(\"Hello, world!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#Create a test method in similar syntax to java\n",
    "def test_vocab(pt_pipeline, deciphered_pipeline, ds):\n",
    "    accuracy = 0\n",
    "    total_count = 0\n",
    "    for i, (text, label) in enumerate(ds.train_data):\n",
    "        pt_text = pt_pipeline(text)\n",
    "        new_text = deciphered_pipeline(text)\n",
    "\n",
    "        # assert pt_text array equals new_text array\n",
    "        accuracy += (pt_text == new_text)\n",
    "        #print(pt_text, new_text)\n",
    "        total_count += 1\n",
    "        if i == 0:\n",
    "            print(new_text)\n",
    "\n",
    "        if (i + 1) % (len(ds.train_data) // 5) == 0:\n",
    "            print(f\"Iteration {i} | Accuracy: {accuracy / total_count} %.\")\n",
    "        if accuracy != total_count:\n",
    "            print(pt_text, new_text)\n",
    "            break\n",
    "    try:\n",
    "        assert(accuracy == total_count)\n",
    "    except AssertionError:\n",
    "        print(\"Not the same\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 6945, 6, 1033, 4, 378, 1, 2, 263, 553]\n",
      "Iteration 1503 | Accuracy: 1.0 %.\n",
      "Iteration 3007 | Accuracy: 1.0 %.\n",
      "Iteration 4511 | Accuracy: 1.0 %.\n",
      "Iteration 6015 | Accuracy: 1.0 %.\n",
      "Iteration 7519 | Accuracy: 1.0 %.\n"
     ]
    }
   ],
   "source": [
    "test_vocab(vocab_from_custom_optimized_pipeline, vocab_from_custom_detailed_pipeline, train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 6945, 6, 1033, 4, 378, 1, 2, 263, 553]\n",
      "Iteration 1503 | Accuracy: 1.0 %.\n",
      "Iteration 3007 | Accuracy: 1.0 %.\n",
      "Iteration 4511 | Accuracy: 1.0 %.\n",
      "Iteration 6015 | Accuracy: 1.0 %.\n",
      "Iteration 7519 | Accuracy: 1.0 %.\n"
     ]
    }
   ],
   "source": [
    "test_vocab(vocab_pipeline, vocab_from_custom_optimized_pipeline, train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}